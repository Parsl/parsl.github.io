---
layout: post
author: Sophie Bui
title: Parsl at SC24
excerpt: Attending SC24 in Atlanta? Drop by these events to see what the Parsl community is up to. 
---
<img src="/images/blog/2024-11-14/SC24-Parsl-Banner.jpg" width="100%" style="border:0px solid black;" alt="A banner graphic with Parsl logo and SC24 logo;">
{: style="text-align: center;"}

SC24, the international conference for high-performance computing, networking, storage, and analysis, will be held November 17‚Äì22 in Atlanta, GA. Several Parsl users and community members will present their work at SC24. See below for information about Parsl-related SC24 activities (all times shown in Eastern Standard Time). Check out the full conference program <a href="https://sc24.conference-program.com/" target="_blank">here</a>.

<hr>

## Monday, November 18
<br>

### Parsl+CWL: Towards Combining the Python and CWL Ecosystems
<pre>üï£ Time: 11:21 ‚Äì 11:44 a.m.   |   ‚ÑπÔ∏è Event Type: Workshop   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB302" target="_blank">B302</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=ws_worksp114&sess=sess756" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: Common Workflow Language (CWL) is a widely adopted language for defining and sharing computational workflows. It is designed to be independent of the execution engine on which workflows are executed. Here, we describe our experiences integrating CWL with Parsl, a Python-based parallel programming library designed to manage execution of workflows across diverse computing environments.

We propose a new method that converts CWL CommandLineTool definitions into Parsl apps, enabling Parsl scripts to easily import and use tools represented in CWL. We describe a Parsl runner capable of executing a CWL CommandLineTool directly. We also describe a proof-of-concept extension to support inline Python in a CWL workflow definition, enabling seamless use in Parsl's Python ecosystem. We demonstrate benefits of this integration by presenting example CWL CommandLineTool definitions that show how they can be used in Parsl, and comparing performance of executing an image processing workflow using Parsl-CWL and other CWL runners.</p>
<br>

<hr>

## Tuesday, November 19
<br>

### Seesaw: Elastic Scaling for Task-Based Distributed Programs
<pre>üïî Time: Noon ‚Äì 5 p.m.   |   ‚ÑπÔ∏è Event Type: Poster   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB302-B305">B302-B305</a>   |   üìù Author: Matthew Chung, UC Riverside </pre>
<p><strong>Description</strong>: Modern batch schedulers in HPC environments enable the shared use of available computational resources via provisioning discrete sets of resources matching user requirements. The lack of elasticity in such scenarios is often addressed using a Pilot job model where multiple separate requests are pooled. In this work, we explore computational elasticity in a popular Python-based workflow system: Parsl. We identify limitations in existing scaling logic and propose a new resource-aware scheduler. We show a significant improvement in the efficiency of compute resources consumed with minimal loss in time to solution.</p>
<br>

### Making Sense of the Chaos: Best Practices for HPC Software Sustainability Strategies and Metrics
<pre>üïî Time: 5:15 ‚Äì 6:45 p.m.   |   ‚ÑπÔ∏è Event Type: BOF   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB210" target="_blank">B210</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=bof181&sess=sess622" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: This BoF brings together participants to build a community and share information and ideas around exploring the challenges in sustainability of HPC software, focusing on 1) defining and measuring research software sustainability metrics and 2) enhancing research software stewardship practices. Participants will discuss software project community health, engineering practices, and funding stability. The session aims to foster collaboration, share insights, and develop actionable strategies for the long-term viability of HPC software projects.</p>
<br>

### Workflows Community: Collaborative Pathways for Designing an Integrated Infrastructure for Research Excellence
<pre>üïî Time: 5:15 ‚Äì 6:45 p.m.   |   ‚ÑπÔ∏è Event Type: BOF   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB212" target="_blank">B212</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=bof206&sess=sess632" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: This session will focus on the integration and scalability of AI-driven scientific workflows across facilities. Building on vibrant discussions from our previous SC BoF sessions, this session will address the challenges and opportunities inherent in multi-facility workflows. Key themes will include the coordination among various computing and experimental facilities, near real-time data processing, and enhancing infrastructure resilience. Participants will engage in collaborative brainstorming sessions to identify innovative solutions for data representation and storage challenges. This session aims to foster an environment of collaboration, driving the development of efficient and scalable workflows that support modern scientific research‚Äôs growing complexity and scale.</p>
<br>

<hr>

## Thursday, November 21
<br>

### Serverless HPC: Challenges, Opportunities, and Future Prospects for Accelerated Cloud Computing
<pre>üï• Time: 10:30 a.m. ‚Äì Noon   |   ‚ÑπÔ∏è Event Type: Panel   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB313B-B314" target="_blank">B313B-B314</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=pan108&sess=sess452" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: With the ongoing convergence of high-performance computing and cloud, HPC gains a chance to transform and improve its runtimes and programming models. HPC systems can increase their efficiency and accessibility by adapting elastic cloud paradigms, with the prime example being serverless computing. Serverless abstracts away resource management and introduces fine-grained allocations, allowing system operators to improve their efficiency with elastic containers (CaaS), functions (FaaS), and acceleration (XaaS). However, adopting serverless technologies brings challenges that have not been treated adequately in HPC: security of multi-tenant deployments, portability, and performance isolation in shared resources.

In this interactive panel, experts from academia and national labs will debate how serverless can support the rigorous demands of HPC applications. They will share experiences of introducing elastic programming models into the rigid world of high-performance systems and outline predictions for the future: Will serverless schedulers become first-class citizens on HPC systems?</p>
<br>

### IRI: What Novel Interfaces Will HPC Expose for Cross-Facility Workflows?
<pre>üïõ Time: 12:15 ‚Äì 1:15 p.m.   |   ‚ÑπÔ∏è Event Type: BOF   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB311" target="_blank">B311</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=bof218&sess=sess670" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: DOE has recently launched the Integrated Research Infrastructure (IRI) program, which is designed to enable new modes of integrated science across DOE user facilities. Common or unified interfaces are needed for these workflows to seamlessly orchestrate resources across high-performance computing, data, and network providers. These interfaces could be REST APIs for programmable workflows, expansive UIs like JupyterLab, or deep integration with external workflow orchestrators.

This BoF will summarize the current efforts of the IRI interfaces working group and individual ASCR user facilities to develop new interfaces. We invite the community to provide feedback to help guide these IRI efforts.</p>
<br>

<hr>

## Friday, November 22
<br>

### Establishing a High-Performance and Productive Ecosystem for Distributed Execution of Python Functions Using Globus Compute
<pre>üï£ Time: 8:40 ‚Äì 9:10 a.m.   |   ‚ÑπÔ∏è Event Type: Workshop   |   üìç Location: <a href="https://sc24.conference-program.com/?post_type=page&p=20&location=rB311" target="_blank">B311</a>   |   üîó <a href="https://sc24.conference-program.com/presentation/?id=ws_hust105&sess=sess771" target="_blank">More details</a>   </pre>
<p><strong>Description</strong>: Democratizing access to the research computing ecosystem is critical for accelerating research progress. However, the gap between a high-level workload, such as Python in a Jupyter notebook, and the resources exposed by HPC systems is significant. Users must securely authenticate, manage network connections, deploy and manage software, provision and configure nodes, and manage workload execution. Globus Compute reduces these barriers by providing a managed, fire-and-forget model that enables execution of Python functions across any resource to which a user has access. In this paper we describe enhancements to Globus Compute that further reduce barriers to use of the research computing ecosystem: an asynchronous, future-based executor interface for submitting and monitoring tasks, shell and MPI-based function types, and a multi-user endpoint that can be deployed by administrators and used by authorized users.</p>
